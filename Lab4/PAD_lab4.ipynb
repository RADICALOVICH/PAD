{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text     label  \\\n0      –ü–æ–º–æ–π–º—É —è –≤–∫—Ä–∞—à–∏–ª–∞—Å—å –≤ –ß–∏–º–∏–Ω–∞ü§ß https://t.co/t2...  positive   \n5                          @buybread_ —è –Ω–µ —Å –ø–æ—Ä—è–¥–∫–µ!!!!  negative   \n10     @ange1flyhigh –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –±—É–¥—É –¥–æ –ø–æ–±–µ–¥–Ω–æ–≥...  positive   \n15     @LimitaVIP –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –≥i–º–Ω...\\r\\n–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ...  negative   \n17                               —è —Å—Ä–∞–ª–∞ –Ω–∞ —ç—Ç—É –±–∏–æ–ª–æ–≥–∏—é  negative   \n...                                                  ...       ...   \n10704  –î–µ–¥—Ä–∞–¥–∏–æ5 —Ç–æ –µ—Å—Ç—å —Ç—ã —Ö–æ—á–µ—à—å —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç—ã –ª–∞...  negative   \n10705  @kmoo_m –î–ï–ô–°–¢–í–ò–¢–ï–õ–¨–ù–û\\r\\n–µ—Å–ª–∏ –ª—é–±–æ–≤—å, —Ç–æ —Ç–æ–ª—å–∫...  positive   \n10708  –° –•–æ–±–∏ —É—Ç—Ä–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–±—Ä—ã–ºüòÇ https://...  positive   \n10711  –ù–µ —É—Å–ø–µ–ª–∞ –≤—Å—Ç–∞—Ç—å, –∞ —É–∂–µ –∑–∞–µ–±–∞–ª–∞—Å—å, –≤–ø—Ä–æ—á–µ–º –Ω–∏—á...  negative   \n10712  –∞—Ö —Ç–≤–∏—Ç—Ç–µ—Ä –∏–≤–∞–Ω–∞ —É—Ä–≥–∞–Ω—Ç–∞, –∞ –≤–µ–¥—å –≤—Å—ë —Ç–∞–∫ —Ö–æ—Ä–æ—à...  negative   \n\n                        id  \n0      1282311169534038016  \n5      1335130757044563971  \n10     1215370396465291267  \n15     1253799540848762887  \n17     1339418979887173632  \n...                    ...  \n10704  1315037255833092098  \n10705  1323606772578459648  \n10708  1310485706213666816  \n10711  1343031810746425344  \n10712  1266838461778100226  \n\n[4569 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>–ü–æ–º–æ–π–º—É —è –≤–∫—Ä–∞—à–∏–ª–∞—Å—å –≤ –ß–∏–º–∏–Ω–∞ü§ß https://t.co/t2...</td>\n      <td>positive</td>\n      <td>1282311169534038016</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>@buybread_ —è –Ω–µ —Å –ø–æ—Ä—è–¥–∫–µ!!!!</td>\n      <td>negative</td>\n      <td>1335130757044563971</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>@ange1flyhigh –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –±—É–¥—É –¥–æ –ø–æ–±–µ–¥–Ω–æ–≥...</td>\n      <td>positive</td>\n      <td>1215370396465291267</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>@LimitaVIP –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –≥i–º–Ω...\\r\\n–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ...</td>\n      <td>negative</td>\n      <td>1253799540848762887</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>—è —Å—Ä–∞–ª–∞ –Ω–∞ —ç—Ç—É –±–∏–æ–ª–æ–≥–∏—é</td>\n      <td>negative</td>\n      <td>1339418979887173632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10704</th>\n      <td>–î–µ–¥—Ä–∞–¥–∏–æ5 —Ç–æ –µ—Å—Ç—å —Ç—ã —Ö–æ—á–µ—à—å —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç—ã –ª–∞...</td>\n      <td>negative</td>\n      <td>1315037255833092098</td>\n    </tr>\n    <tr>\n      <th>10705</th>\n      <td>@kmoo_m –î–ï–ô–°–¢–í–ò–¢–ï–õ–¨–ù–û\\r\\n–µ—Å–ª–∏ –ª—é–±–æ–≤—å, —Ç–æ —Ç–æ–ª—å–∫...</td>\n      <td>positive</td>\n      <td>1323606772578459648</td>\n    </tr>\n    <tr>\n      <th>10708</th>\n      <td>–° –•–æ–±–∏ —É—Ç—Ä–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–±—Ä—ã–ºüòÇ https://...</td>\n      <td>positive</td>\n      <td>1310485706213666816</td>\n    </tr>\n    <tr>\n      <th>10711</th>\n      <td>–ù–µ —É—Å–ø–µ–ª–∞ –≤—Å—Ç–∞—Ç—å, –∞ —É–∂–µ –∑–∞–µ–±–∞–ª–∞—Å—å, –≤–ø—Ä–æ—á–µ–º –Ω–∏—á...</td>\n      <td>negative</td>\n      <td>1343031810746425344</td>\n    </tr>\n    <tr>\n      <th>10712</th>\n      <td>–∞—Ö —Ç–≤–∏—Ç—Ç–µ—Ä –∏–≤–∞–Ω–∞ —É—Ä–≥–∞–Ω—Ç–∞, –∞ –≤–µ–¥—å –≤—Å—ë —Ç–∞–∫ —Ö–æ—Ä–æ—à...</td>\n      <td>negative</td>\n      <td>1266838461778100226</td>\n    </tr>\n  </tbody>\n</table>\n<p>4569 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('rusentitweet_train.csv')\n",
    "df_test = pd.read_csv('rusentitweet_test.csv')\n",
    "\n",
    "#–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ label = positive/negative\n",
    "df_train = df_train[df_train['label'].isin(['positive', 'negative'])]\n",
    "df_test = df_test[df_test['label'].isin(['positive', 'negative'])]\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)      # –£–¥–∞–ª–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)      # –£–¥–∞–ª–µ–Ω–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
    "    tweet = re.sub(r'\\d+', '', tweet)       # –£–¥–∞–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)   # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "    tweet = re.sub(r'[a-zA-Z]+', '', tweet) # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü–µ\n",
    "    tweet = re.sub(r'[^\\w\\s,]', '', tweet)  # –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏\n",
    "    tweet = tweet.lower()                   # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    tweet = \" \".join([word for word in tweet.split() if word not in russian_stopwords]) # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip() # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    tweet = re.sub(r'\\b\\w\\b', '', tweet)   # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –±—É–∫–≤—ã\n",
    "    return tweet\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ –∫ –∫–∞–∂–¥–æ–º—É —Ç–≤–∏—Ç—É –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "df_train['text'] = df_train['text'].apply(clean_tweet)\n",
    "df_test['text'] = df_test['text'].apply(clean_tweet)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ –∫ –∫–∞–∂–¥–æ–º—É —Ç–≤–∏—Ç—É –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "df_train['text'] = df_train['text'].apply(clean_tweet)\n",
    "df_test['text'] = df_test['text'].apply(clean_tweet)\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ç–≤–∏—Ç–æ–≤\n",
    "df_train = df_train[df_train['text'].str.strip().astype(bool)]\n",
    "df_test = df_test[df_test['text'].str.strip().astype(bool)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text     label  \\\n0                                     –ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω  positive   \n5                                                 –ø–æ—Ä—è–¥–∫  negative   \n10                   —Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á  positive   \n15         —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω  negative   \n17                                           —Å—Ä–∞–ª –±–∏–æ–ª–æ–≥  negative   \n...                                                  ...       ...   \n10704  –¥–µ–¥—Ä–∞–¥ —Ö–æ—á–µ—à —Å–∫–∞–∑–∞ –ª–∞–π–∫–∞ –º–æ —Ç–≤–∏—Ç –ø–∑–¥—Ü —É–¥–∞–ª—è –æ—Ç—Å—é–¥  negative   \n10705                             –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω –ª—é–±–æ–≤ —Ç–∞–∫  positive   \n10708                                       —Ö–æ–± —É—Ç—Ä –¥–æ–±—Ä  positive   \n10711                              —É—Å–ø–µ–ª –≤—Å—Ç–∞—Ç –∑–∞–µ–±–∞ –Ω–æ–≤  negative   \n10712        –∞—Ö —Ç–≤–∏—Ç—Ç–µ—Ä –∏–≤–∞ —É—Ä–≥–∞–Ω—Ç –≤—Å–µ –Ω–∞—á–∏–Ω–∞ –≤—Å–µ –ø–æ—Ç–µ—Ä—è  negative   \n\n                        id  \n0      1282311169534038016  \n5      1335130757044563971  \n10     1215370396465291267  \n15     1253799540848762887  \n17     1339418979887173632  \n...                    ...  \n10704  1315037255833092098  \n10705  1323606772578459648  \n10708  1310485706213666816  \n10711  1343031810746425344  \n10712  1266838461778100226  \n\n[4536 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>–ø–æ–º–æ–π–º –≤–∫—Ä–∞—à —á–∏–º–∏–Ω</td>\n      <td>positive</td>\n      <td>1282311169534038016</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>–ø–æ—Ä—è–¥–∫</td>\n      <td>negative</td>\n      <td>1335130757044563971</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>—Å–ª–µ–¥ –±—É–¥ –ø–æ–±–µ–¥–Ω –∑–∞–∫—Ä—ã–≤–∞ –ø–æ–∂–µ–ª–∞ —É–¥–∞—á</td>\n      <td>positive</td>\n      <td>1215370396465291267</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –≥–º–Ω —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω –ø–æ–∫ –µ—â —Å–¥–æ—Ö–ª —É–∫—Ä–∞–∏–Ω</td>\n      <td>negative</td>\n      <td>1253799540848762887</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>—Å—Ä–∞–ª –±–∏–æ–ª–æ–≥</td>\n      <td>negative</td>\n      <td>1339418979887173632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10704</th>\n      <td>–¥–µ–¥—Ä–∞–¥ —Ö–æ—á–µ—à —Å–∫–∞–∑–∞ –ª–∞–π–∫–∞ –º–æ —Ç–≤–∏—Ç –ø–∑–¥—Ü —É–¥–∞–ª—è –æ—Ç—Å—é–¥</td>\n      <td>negative</td>\n      <td>1315037255833092098</td>\n    </tr>\n    <tr>\n      <th>10705</th>\n      <td>–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω –ª—é–±–æ–≤ —Ç–∞–∫</td>\n      <td>positive</td>\n      <td>1323606772578459648</td>\n    </tr>\n    <tr>\n      <th>10708</th>\n      <td>—Ö–æ–± —É—Ç—Ä –¥–æ–±—Ä</td>\n      <td>positive</td>\n      <td>1310485706213666816</td>\n    </tr>\n    <tr>\n      <th>10711</th>\n      <td>—É—Å–ø–µ–ª –≤—Å—Ç–∞—Ç –∑–∞–µ–±–∞ –Ω–æ–≤</td>\n      <td>negative</td>\n      <td>1343031810746425344</td>\n    </tr>\n    <tr>\n      <th>10712</th>\n      <td>–∞—Ö —Ç–≤–∏—Ç—Ç–µ—Ä –∏–≤–∞ —É—Ä–≥–∞–Ω—Ç –≤—Å–µ –Ω–∞—á–∏–Ω–∞ –≤—Å–µ –ø–æ—Ç–µ—Ä—è</td>\n      <td>negative</td>\n      <td>1266838461778100226</td>\n    </tr>\n  </tbody>\n</table>\n<p>4536 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def stem_tweet(tweet):\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–≤–∏—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "    words = tweet.split()\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–µ–º–º–∏–Ω–≥ –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "df_train_stem = df_train.copy()\n",
    "df_test_stem = df_test.copy()\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å—Ç–µ–º–º–∏–Ω–≥–∞ –∫ –∫–∞–∂–¥–æ–º—É —Ç–≤–∏—Ç—É\n",
    "df_train_stem['text'] = df_train_stem['text'].apply(stem_tweet)\n",
    "df_test_stem['text'] = df_test_stem['text'].apply(stem_tweet)\n",
    "\n",
    "df_train_stem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏—Ö –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤\n",
    "X_train = vectorizer.fit_transform(df_train_stem['text'])\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤\n",
    "X_test = vectorizer.transform(df_test_stem['text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏—Ö\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.93      0.82       656\n",
      "    positive       0.84      0.54      0.66       477\n",
      "\n",
      "    accuracy                           0.76      1133\n",
      "   macro avg       0.79      0.73      0.74      1133\n",
      "weighted avg       0.78      0.76      0.75      1133\n",
      "\n",
      "–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.88      0.80       656\n",
      "    positive       0.77      0.55      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1133\n",
      "   macro avg       0.75      0.72      0.72      1133\n",
      "weighted avg       0.75      0.74      0.73      1133\n",
      "\n",
      "ROC_AUC_SCORE –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è c —ç–º–æ–¥–∑–∏:\n",
      " 0.7320444725673672\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "logreg = LogisticRegression(random_state=RANDOM_SEED)\n",
    "logreg.fit(X_train_tfidf, df_train['label'])\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞\n",
    "rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "rf.fit(X_train_tfidf, df_train['label'])\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "logreg_pred = logreg.predict(X_test_tfidf)\n",
    "rf_pred = rf.predict(X_test_tfidf)\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(\"–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:\\n\", classification_report(df_test['label'], logreg_pred))\n",
    "print(\"–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å:\\n\", classification_report(df_test['label'], rf_pred))\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels in df_test_lem to numeric\n",
    "numeric_labels = [0 if label == 'negative' else 1 for label in df_test_stem['label']]\n",
    "\n",
    "# Convert predictions in logreg_pred_lem to numeric\n",
    "numeric_predictions = [0 if pred == 'negative' else 1 for pred in logreg_pred]\n",
    "\n",
    "roc_auc = roc_auc_score(numeric_labels, numeric_predictions)\n",
    "print(\"ROC_AUC_SCORE –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è c —ç–º–æ–¥–∑–∏:\\n\", roc_auc)\n",
    "logreg_pred.shape()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\n",
      "–ª—é–±–ª –∫—Ä–∞—Å–∏–≤ –ø—Ä–µ–∫—Ä–∞—Å–Ω –ª—É—á—à –º–∏–ª –∫–ª–∞—Å—Å–Ω –∫—Ä—É—Ç –Ω—Ä–∞–≤ –≤–∞ —Ä–∞–¥\n",
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\n",
      "–±–ª—è—Ç –ø–∏–∑–¥–µ—Ü –Ω–∞—Ö —Å—É–∫ —Ö—É–π–Ω –≤–æ–æ–±—â –∑–∞–µ–±–∞ –Ω–µ–Ω–∞–≤–∏–∂ –≥—Ä—É—Å—Ç–Ω —É–∂–∞—Å–Ω\n",
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞:\n",
      "–ª—é–±–ª –∫—Ä–∞—Å–∏–≤ –±–ª—è—Ç –ø—Ä–µ–∫—Ä–∞—Å–Ω –∫—Ä—É—Ç –ª—É—á—à –º–∏–ª —ç—Ç –∫–ª–∞—Å—Å–Ω –ø–∏–∑–¥–µ—Ü –Ω—Ä–∞–≤ —Ö–æ—Ä–æ—à —Ä–∞–¥ –≤–∞ —Ö–æ—á —Å—É–∫ –æ—á–µ–Ω –ª—é–±–∏–º –æ–±–æ–∂–∞ –Ω–∞—Ö\n"
     ]
    }
   ],
   "source": [
    "# –î–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "logreg_coef = logreg.coef_[0]\n",
    "sorted_features = sorted(zip(logreg_coef, feature_names), reverse=True)\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\")\n",
    "sorted_words = [word for coef, word in sorted_features]\n",
    "sorted_words_string = ' '.join(sorted_words[:10])\n",
    "print(sorted_words_string)\n",
    "\n",
    "sorted_features = sorted(zip(logreg_coef, feature_names))\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\")\n",
    "sorted_words = [word for coef, word in sorted_features]\n",
    "sorted_words_string = ' '.join(sorted_words[:10])\n",
    "print(sorted_words_string)\n",
    "\n",
    "# –î–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞\n",
    "rf_importances = rf.feature_importances_\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
    "sorted_indices = np.argsort(rf_importances)[::-1]\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞\n",
    "top_words = [feature_names[i] for i in sorted_indices[:20]]\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫—É, —Ä–∞–∑–¥–µ–ª—è—è —Å–ª–æ–≤–∞ –ø—Ä–æ–±–µ–ª–∞–º–∏\n",
    "top_words_string = ' '.join(top_words)\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞:\")\n",
    "print(top_words_string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Mystem\n",
    "mystem = Mystem()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é –∫ —Ç–µ–∫—Å—Ç—É –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    return ''.join(lemmas).strip()\n",
    "\n",
    "df_train_lem = df_train.copy()\n",
    "df_test_lem = df_test.copy()\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫ —Å—Ç–æ–ª–±—Ü—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤ DataFrame\n",
    "df_train_lem['text'] = df_train_lem['text'].apply(lemmatize_text)\n",
    "df_test_lem['text'] = df_test_lem['text'].apply(lemmatize_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏—Ö –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤\n",
    "X_train_lem = vectorizer.fit_transform(df_train_lem['text'])\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤\n",
    "X_test_lem = vectorizer.transform(df_test_lem['text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏—Ö\n",
    "X_train_tfidf_lem = tfidf_transformer.fit_transform(X_train_lem)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "X_test_tfidf_lem = tfidf_transformer.transform(X_test_lem)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.92      0.82       656\n",
      "    positive       0.84      0.56      0.67       477\n",
      "\n",
      "    accuracy                           0.77      1133\n",
      "   macro avg       0.79      0.74      0.75      1133\n",
      "weighted avg       0.79      0.77      0.76      1133\n",
      "\n",
      "–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.87      0.79       656\n",
      "    positive       0.76      0.55      0.63       477\n",
      "\n",
      "    accuracy                           0.74      1133\n",
      "   macro avg       0.74      0.71      0.71      1133\n",
      "weighted avg       0.74      0.74      0.73      1133\n",
      "\n",
      "ROC_AUC_SCORE –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –±–µ–∑ —ç–º–æ–¥–∑–∏:\n",
      " 0.7428126757682671\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "logreg = LogisticRegression(random_state=RANDOM_SEED)\n",
    "logreg.fit(X_train_tfidf_lem, df_train_lem['label'])\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞\n",
    "rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "rf.fit(X_train_tfidf_lem, df_train_lem['label'])\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "logreg_pred_lem = logreg.predict(X_test_tfidf_lem)\n",
    "rf_pred_lem = rf.predict(X_test_tfidf_lem)\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(\"–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:\\n\", classification_report(df_test_lem['label'], logreg_pred_lem))\n",
    "print(\"–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å:\\n\", classification_report(df_test_lem['label'], rf_pred_lem))\n",
    "\n",
    "# Convert labels in df_test_lem to numeric\n",
    "numeric_labels = [0 if label == 'negative' else 1 for label in df_test_lem['label']]\n",
    "\n",
    "# Convert predictions in logreg_pred_lem to numeric\n",
    "numeric_predictions = [0 if pred == 'negative' else 1 for pred in logreg_pred_lem]\n",
    "\n",
    "roc_auc = roc_auc_score(numeric_labels, numeric_predictions)\n",
    "print(\"ROC_AUC_SCORE –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –±–µ–∑ —ç–º–æ–¥–∑–∏:\\n\", roc_auc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\n",
      "–ª—é–±–∏—Ç—å —Ö–æ—Ä–æ—à–∏–π –∫—Ä–∞—Å–∏–≤—ã–π –º–∏–ª—ã–π –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –∫–ª–∞—Å—Å–Ω—ã–π –≤–∞—É –Ω—Ä–∞–≤–∏—Ç—å—Å—è –∫—Ä—É—Ç–æ–π –ª—é–±–æ–≤—å\n",
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\n",
      "–±–ª—è—Ç—å –ø–∏–∑–¥–µ—Ü –Ω–∞—Ö—É–π —Å—É–∫–∞ —É–º–∏—Ä–∞—Ç—å –≤–æ–æ–±—â–µ —Ö—É–π–Ω—è –Ω–µ–Ω–∞–≤–∏–¥–µ—Ç—å —É—Ö–æ–¥–∏—Ç—å —Å–¥—ã—Ö–∞—Ç—å\n",
      "–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞:\n",
      "–ª—é–±–∏—Ç—å —Ö–æ—Ä–æ—à–∏–π –±–ª—è—Ç—å –∫—Ä–∞—Å–∏–≤—ã–π —ç—Ç–æ –ø–∏–∑–¥–µ—Ü –Ω—Ä–∞–≤–∏—Ç—å—Å—è –≤–∞—É –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π —Ö–æ—Ç–µ—Ç—å –ª—é–±–æ–≤—å –º–∏–ª—ã–π –∫–ª–∞—Å—Å–Ω—ã–π –∫—Ä—É—Ç–æ–π —Å–∞–º—ã–π –æ—á–µ–Ω—å –≤–æ–æ–±—â–µ –∫—Ä–∞—Å–∏–≤–æ —É–º–∏—Ä–∞—Ç—å —Ä–∞–¥\n"
     ]
    }
   ],
   "source": [
    "# –î–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "logreg_coef = logreg.coef_[0]\n",
    "sorted_features = sorted(zip(logreg_coef, feature_names), reverse=True)\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\")\n",
    "sorted_words = [word for coef, word in sorted_features]\n",
    "sorted_words_string = ' '.join(sorted_words[:10])\n",
    "print(sorted_words_string)\n",
    "\n",
    "sorted_features = sorted(zip(logreg_coef, feature_names))\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:\")\n",
    "sorted_words = [word for coef, word in sorted_features]\n",
    "sorted_words_string = ' '.join(sorted_words[:10])\n",
    "print(sorted_words_string)\n",
    "\n",
    "# –î–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞\n",
    "rf_importances = rf.feature_importances_\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
    "sorted_indices = np.argsort(rf_importances)[::-1]\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞\n",
    "top_words = [feature_names[i] for i in sorted_indices[:20]]\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫—É, —Ä–∞–∑–¥–µ–ª—è—è —Å–ª–æ–≤–∞ –ø—Ä–æ–±–µ–ª–∞–º–∏\n",
    "top_words_string = ' '.join(top_words)\n",
    "print(\"–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞:\")\n",
    "print(top_words_string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
